{
  "publications": [
    {
      "title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks",
      "conference": "CVPR'25 (But 1 of 19)",
      "authors": "<strong>Shiduo Zhang</strong>, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang & Xipeng Qiu",
      "description": "\"First benchmark for VLA. Not only for evaluation, definition matters most.\"",
      "links": [
        {"type": "Paper", "url": "https://arxiv.org/abs/2412.18194"},
        {"type": "Project Page", "url": "https://vlabench.github.io/"},
        {"type": "Code", "url": "https://github.com/OpenMOSS/VLABench"}
      ],
      "image": "./media/main_page.jpg"
    },
    {
      "title": "LongSafety: Enhance Safety for Long-Context LLMs",
      "authors": "Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, <strong>Zhe Xu</strong>, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang",
      "description": "This paper introduces LongSafety, a specialized safety alignment dataset for long-context LLMs (spanning 8 tasks, ~17K samples, ~40.9K tokens average) along with a benchmark LongSafetyBench, and shows that fine-tuning on it improves safety in long contexts without degrading general capabilities.",
      "links": [
        {"type": "Paper", "url": "https://arxiv.org/abs/2411.06899v1"},
        {"type": "Hugging Face", "url": "https://huggingface.co/datasets/LutherXD/LongSafetyBench"},
        {"type": "Code", "url": "https://github.com/OpenMOSS/LongSafety"}
      ],
      "image": "./media/LSB.png"
    },
    {
      "title": "DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels",
      "conference": "ICLR2025",
      "authors": "<strong>Zhe Xu</strong>, Jiasheng Ye, Xiaoran Liu, Xiangyang Liu, Tianxiang Sun, Zhigeng Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, Xipeng Qiu",
      "description": "DetectiveQA is a narrative reasoning benchmark based on detective novels with very long contexts (on average over 100,000 tokens), offering 1,200 Chinese-English question-answer pairs with detailed reasoning chains, designed to test LLMsâ€™ abilities in long-text comprehension, evidence retrieval, and reasoning, and demonstrating that current long-context LLMs still struggle significantly with dependency questions that truly require full context.",
      "links": [
        {"type": "Paper", "url": "https://arxiv.org/abs/2409.02465"},
        {"type": "Code", "url": "https://github.com/Phospheneser/DetectiveQA"},
        {"type": "Hugging Face", "url": "https://huggingface.co/datasets/Phospheneser/DetectiveQA"}
      ],
      "image": "./media/detectiveqa.png"
    }
  ]
}
